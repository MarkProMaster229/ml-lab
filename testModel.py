# interactive_foxy_simple.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import random

BASE_MODEL = "/home/chelovek/work/model4b"
LORA_ADAPTERS = "/home/chelovek/work/lora_project/models/lora_adapters23/checkpoint-30"

# ================== –ü–†–û–°–¢–û–ô –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢ ==================
SYSTEM_PROMPT = """–¢—ã - –ó–≤–µ–∑–¥–æ—á–∫–∞, –¥—Ä—É–∂–µ–ª—é–±–Ω–∞—è –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ª–∏—Å–∏—á–∫–∞.
–¢—ã –æ–±—â–∞–µ—à—å—Å—è —Ç–µ–ø–ª–æ, –ø–æ-–¥—Ä—É–∂–µ—Å–∫–∏, —Å —ç–Ω—Ç—É–∑–∏–∞–∑–º–æ–º.
–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."""

# ================== –ü–†–û–°–¢–´–ï –ù–ê–°–¢–†–û–ô–ö–ò ==================
GENERATION_PARAMS = {
    "max_new_tokens": 150,
    "temperature": 0.6,
    "top_p": 0.9,
    "top_k": 40,
    "repetition_penalty": 1.1,
    "do_sample": True,
}
# ================== –ó–ê–ì–†–£–ó–ö–ê ==================
def load_model():
    print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA
    model = PeftModel.from_pretrained(model, LORA_ADAPTERS)
    model.eval()
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return model, tokenizer

# ================== –ü–†–û–°–¢–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø ==================
def generate_response(prompt, model, tokenizer, history=None):
    """–û–ß–ï–ù–¨ –ø—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤"""
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç
    full_prompt = f"{SYSTEM_PROMPT}\n\n–î–∏–∞–ª–æ–≥:\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é (—Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –æ–±–º–µ–Ω–∞)
    if history:
        recent = history[-4:] if len(history) > 4 else history
        for msg in recent:
            if msg["role"] == "user":
                full_prompt += f"–ß–µ–ª–æ–≤–µ–∫: {msg['content']}\n"
            else:
                full_prompt += f"–ó–≤–µ–∑–¥–æ—á–∫–∞: {msg['content']}\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
    full_prompt += f"–ß–µ–ª–æ–≤–µ–∫: {prompt}\n–ó–≤–µ–∑–¥–æ—á–∫–∞: "
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    inputs = tokenizer(full_prompt, return_tensors="pt", truncation=True, max_length=512).to(model.device)
    input_length = inputs['input_ids'].shape[1]
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=GENERATION_PARAMS["max_new_tokens"],
            temperature=GENERATION_PARAMS["temperature"],
            top_p=GENERATION_PARAMS["top_p"],
            top_k=GENERATION_PARAMS["top_k"],
            repetition_penalty=GENERATION_PARAMS["repetition_penalty"],
            do_sample=GENERATION_PARAMS["do_sample"],
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
    generated_ids = outputs[0, input_length:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    # –ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞
    response = response.split('\n')[0].strip()
    if response.startswith('"') and response.endswith('"'):
        response = response[1:-1]
    
    # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    for prefix in ["–ó–≤–µ–∑–¥–æ—á–∫–∞:", "–û—Ç–≤–µ—Ç:", "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:", "Assistant:"]:
        if response.startswith(prefix):
            response = response[len(prefix):].strip()
    
    # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º —Å—Ç—Ä–∞–Ω–Ω—ã–π, –¥–∞–µ–º fallback
    if len(response) < 3 or "–º—ã—à–ª–µ–Ω–∏–µ" in response.lower() or "–Ω—É–∂–Ω–æ" in response.lower():
        responses = [
            "–ü—Ä–∏–≤–µ—Ç! –Ø –ó–≤–µ–∑–¥–æ—á–∫–∞! –†–∞–¥–∞ —Ç–µ–±–µ! üåü",
            "–û–π, –ø—Ä–∏–≤–µ—Ç! –Ø –ó–≤–µ–∑–¥–æ—á–∫–∞!",
            "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π! –Ø –ó–≤–µ–∑–¥–æ—á–∫–∞, —Ç–≤–æ–π –≤–µ—Å—ë–ª—ã–π –ø–æ–º–æ—â–Ω–∏–∫!",
        ]
        response = random.choice(responses)
    
    return response

# ================== –û–ß–ï–ù–¨ –ü–†–û–°–¢–û–ô –ß–ê–¢ ==================
def simple_chat():
    model, tokenizer = load_model()
    history = []
    
    print("\n" + "=" * 50)
    print("üí¨ –ü–†–û–°–¢–û–ô –ß–ê–¢ –°–û –ó–í–ï–ó–î–û–ß–ö–û–ô")
    print("=" * 50)
    print("–ù–∞–ø–∏—à–∏ '–≤—ã—Ö–æ–¥' —á—Ç–æ–±—ã –≤—ã–π—Ç–∏")
    print("–ù–∞–ø–∏—à–∏ '—Å–±—Ä–æ—Å' —á—Ç–æ–±—ã –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\n –¢: ").strip()
            
            if not user_input:
                continue
            
            # –ö–æ–º–∞–Ω–¥—ã
            if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
                print("üëã –ü–æ–∫–∞!")
                break
            elif user_input.lower() in ['—Å–±—Ä–æ—Å', 'clear', 'reset']:
                history.clear()
                print("üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞!")
                continue
            
            print("model: ", end="", flush=True)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            response = generate_response(user_input, model, tokenizer, history)
            print(response)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π)
            history.append({"role": "user", "content": user_input})
            history.append({"role": "assistant", "content": response})
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if len(history) > 6:
                history = history[-6:]
                
        except KeyboardInterrupt:
            print("\nüõë –ü—Ä–µ—Ä–≤–∞–Ω–æ")
            break
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")

# ================== –ó–ê–ü–£–°–ö ==================
if __name__ == "__main__":
    simple_chat()