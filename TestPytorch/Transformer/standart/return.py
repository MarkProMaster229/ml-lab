import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import os
from torch.optim.lr_scheduler import ReduceLROnPlateau
# ----------------------------
# 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# ----------------------------
class TransformerBlock(nn.Module):
    def __init__(self, sizeVector=64, numHeads=2):
        super().__init__()
        self.ln1 = nn.LayerNorm(sizeVector)
        self.attn = nn.MultiheadAttention(sizeVector, numHeads, batch_first=True)
        self.ln2 = nn.LayerNorm(sizeVector)
        self.ff = nn.Sequential(
            nn.Linear(sizeVector, sizeVector*4),
            nn.GELU(),
            nn.Linear(sizeVector*4, sizeVector),
        )

    def forward(self, x, attMask=None):
        h = self.ln1(x)
        z, _ = self.attn(h, h, h, attn_mask=attMask)
        h = self.ln2(x)
        z1 = self.ff(h)
        return x + z1

class TransformerRun(nn.Module):
    def __init__(self, vocabSize=120000, maxLong=200, sizeVector=64, block=2):
        super().__init__()
        self.maxLong = maxLong
        self.tokenEmbed = nn.Embedding(vocabSize, sizeVector)
        self.posEmbed = nn.Embedding(maxLong, sizeVector)
        self.ln_f = nn.LayerNorm(sizeVector)
        self.layers = nn.ModuleList([TransformerBlock(sizeVector=sizeVector) for _ in range(block)])
        self.lmHead = nn.Linear(sizeVector, vocabSize)

    def forward(self, x):
        B, T = x.shape
        tok = self.tokenEmbed(x)
        pos = self.posEmbed(torch.arange(T, device=x.device)).unsqueeze(0).repeat(B, 1, 1)
        h = tok + pos
        attMask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()
        for layer in self.layers:
            h = layer(h, attMask=attMask)
        h = self.ln_f(h)
        return self.lmHead(h)

# ----------------------------
# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
# ----------------------------
path = "/home/chelovek/exper/trained_model60"
config = torch.load(f"{path}/config.pth")
tokenizer = AutoTokenizer.from_pretrained(path)

model = TransformerRun(
    vocabSize=config['vocabSize'],
    maxLong=config['maxLong'],
    sizeVector=config['sizeVector'],
    block=config['numLayers']
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.load_state_dict(torch.load(f"{path}/model_weights.pth", map_location=device))

optimizer = optim.Adam(model.parameters(), lr=config.get('lr', 1e-4))
optimizer.load_state_dict(torch.load(f"{path}/optimizer.pth", map_location=device))

model.train()

# ----------------------------
# 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º
# ----------------------------
dataset_path = "/home/chelovek/gatasetexp/simple.json"
with open(dataset_path, "r", encoding="utf-8") as f:
    new_dataset = [line.strip() for line in f if line.strip()]

class MyDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length):
        self.data = [tokenizer.encode(t, truncation=True, max_length=max_length) for t in texts]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return torch.tensor(self.data[idx], dtype=torch.long)

def collate_fn(batch):
    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –¥–ª–∏–Ω–µ –±–∞—Ç—á–∞
    return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)

dataset = MyDataset(new_dataset, tokenizer, config['maxLong'])
dataloader = DataLoader(dataset, batch_size=12, shuffle=True, collate_fn=collate_fn)
# ----------------------------
# 4. –¶–∏–∫–ª –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å –ª–æ–≥–∞–º–∏ –ø–æ —ç–ø–æ—Ö–∞–º –∏ ReduceLROnPlateau
# ----------------------------
loss_fn = nn.CrossEntropyLoss()
epochs = 10       # –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —ç–ø–æ—Ö
save_every = 5    # —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö
save_path = "/home/chelovek/exper/newtrainedModel"
os.makedirs(save_path, exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–∏–º scheduler
scheduler = ReduceLROnPlateau(
    optimizer, 
    mode='min',       # —É–º–µ–Ω—å—à–∞–µ–º LR, –∫–æ–≥–¥–∞ loss –Ω–µ –ø–∞–¥–∞–µ—Ç
    factor=0.5,       # LR –±—É–¥–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å—Å—è –≤ 2 —Ä–∞–∑–∞
    patience=2,       # –∂–¥–∞—Ç—å 2 —ç–ø–æ—Ö–∏ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    verbose=True
)

num_batches = len(dataloader)
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –∑–∞ —ç–ø–æ—Ö—É: {num_batches}")

for epoch in range(epochs):
    epoch_loss = 0.0
    for batch in dataloader:
        batch = batch.to(device)
        optimizer.zero_grad()
        logits = model(batch)
        loss = loss_fn(logits.view(-1, logits.size(-1)), batch.view(-1))
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
    
    # –°—Ä–µ–¥–Ω–∏–π loss –∑–∞ —ç–ø–æ—Ö—É
    avg_epoch_loss = epoch_loss / num_batches
    
    # –û–±–Ω–æ–≤–ª—è–µ–º LR –ø–æ ReduceLROnPlateau
    scheduler.step(avg_epoch_loss)
    
    # –í—ã–≤–æ–¥–∏–º –ª–æ–≥: —ç–ø–æ—Ö–∞, —Å—Ä–µ–¥–Ω–∏–π loss –∏ —Ç–µ–∫—É—â–∏–π LR
    current_lr = optimizer.param_groups[0]['lr']
    print(f"[–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}] Avg Loss: {avg_epoch_loss:.4f}, LR: {current_lr:.6f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
    if (epoch + 1) % save_every == 0:
        epoch_save_path = os.path.join(save_path, f"epoch_{epoch+1}")
        os.makedirs(epoch_save_path, exist_ok=True)
        torch.save(model.state_dict(), f"{epoch_save_path}/model_weights.pth")
        torch.save(optimizer.state_dict(), f"{epoch_save_path}/optimizer.pth")
        torch.save(config, f"{epoch_save_path}/config.pth")
        tokenizer.save_pretrained(epoch_save_path)
        print(f"üíæ –ú–æ–¥–µ–ª—å –ø–æ—Å–ª–µ {epoch+1} —ç–ø–æ—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {epoch_save_path}")

# ----------------------------
# 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
# ----------------------------
torch.save(model.state_dict(), f"{path}/model_weights.pth")
torch.save(optimizer.state_dict(), f"{path}/optimizer.pth")
torch.save(config, f"{path}/config.pth")
tokenizer.save_pretrained(path)
print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}")
