#This file is entirely AI generated.
#ÑÑ‚Ð¾Ñ‚ Ñ„Ð°Ð¹Ð» Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð˜Ð˜.

import torch
import argparse
from decoderOnly import TransformerRun
from transformers import AutoTokenizer

class ChatBot:
    def __init__(self, model_path="trained_model"):
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ"""
        print(f"ðŸ¤– Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð· {model_path}...")
        
        # 1. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ (Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾)
        self.config = torch.load(f"{model_path}/config.pth", weights_only=True)
        print(f"ðŸ“Š ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {self.config}")
        
        # 2. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # 3. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ Ñ‚ÐµÐ¼Ð¸ Ð¶Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸
        self.model = TransformerRun(
            vocabSize=self.config['vocabSize'],
            maxLong=self.config['maxLong'],
            sizeVector=self.config['sizeVector'],
            block=self.config['numLayers']
        )
        
        # 4. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð²ÐµÑÐ° (Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾)
        self.model.load_state_dict(
            torch.load(f"{model_path}/model_weights.pth", 
                      map_location='cpu', weights_only=True)
        )
        
        # 5. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()  # Ð ÐµÐ¶Ð¸Ð¼ Ð¾Ñ†ÐµÐ½ÐºÐ¸
        
        print("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°!")
        print(f"ðŸ’» Ð£ÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾: {self.device}")
        print(f"ðŸ“š Ð Ð°Ð·Ð¼ÐµÑ€ ÑÐ»Ð¾Ð²Ð°Ñ€Ñ: {self.config['vocabSize']}")
        print(f"ðŸ”¤ maxLong: {self.config['maxLong']}")
    
    def generate(self, prompt, max_length=100, temperature=0.7, top_k=50):
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ð° Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚"""
        # 1. Ð¢Ð¾ÐºÐµÐ½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=min(self.config['maxLong'] - max_length, 512)
        )
        input_ids = inputs["input_ids"].to(self.device)
        
        # 2. Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ñ‹
        generated_ids = input_ids.clone()
        
        with torch.no_grad():  # ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð²
            for _ in range(max_length):
                # ÐŸÑ€ÑÐ¼Ð¾Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´
                outputs = self.model(generated_ids)
                
                # Ð‘ÐµÑ€ÐµÐ¼ Ð»Ð¾Ð³Ð¸Ñ‚Ñ‹ Ð´Ð»Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð°
                next_token_logits = outputs[0, -1, :] / temperature
                
                # Top-k sampling (ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾)
                if top_k > 0:
                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                    next_token_logits[indices_to_remove] = -float('Inf')
                
                # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ softmax Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹
                probs = torch.softmax(next_token_logits, dim=-1)
                
                # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ñ‚Ð¾ÐºÐµÐ½
                next_token = torch.multinomial(probs, num_samples=1)
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ðº ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
                generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0)], dim=1)
                
                # ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ÑÑ Ð½Ð° ÑÐ¿ÐµÑ†-Ñ‚Ð¾ÐºÐµÐ½Ð°Ñ…
                if self.tokenizer.eos_token_id and next_token.item() == self.tokenizer.eos_token_id:
                    break
                if self.tokenizer.sep_token_id and next_token.item() == self.tokenizer.sep_token_id:
                    break
                if next_token.item() == self.tokenizer.pad_token_id:
                    break
        
        # 3. Ð”ÐµÐºÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð² Ñ‚ÐµÐºÑÑ‚
        full_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        
        # ÐžÑ‚Ð´ÐµÐ»ÑÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
        if prompt in full_text:
            response = full_text[len(prompt):].strip()
        else:
            response = full_text.strip()
        
        return response
    
    def interactive_chat(self):
        """Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ‡Ð°Ñ‚ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ Ð² Ð±ÐµÑÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾Ð¼ Ñ†Ð¸ÐºÐ»Ðµ"""
        print("\n" + "="*60)
        print("ðŸ¤– Ð§ÐÐ¢-Ð‘ÐžÐ¢ Ð—ÐÐŸÐ£Ð©Ð•Ð!".center(60))
        print("="*60)
        print("ðŸ“ ÐšÐ¾Ð¼Ð°Ð½Ð´Ñ‹:")
        print("  /exit, /quit, /q - Ð²Ñ‹Ð¹Ñ‚Ð¸ Ð¸Ð· Ñ‡Ð°Ñ‚Ð°")
        print("  /clear - Ð¾Ñ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ")
        print("  /temp X - ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñƒ (0.1-2.0)")
        print("  /len X - ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð´Ð»Ð¸Ð½Ñƒ Ð¾Ñ‚Ð²ÐµÑ‚Ð° (10-200)")
        print("  /topk X - ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ top-k sampling (0-100)")
        print("="*60)
        
        # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
        temperature = 0.7
        max_length = 100
        top_k = 50
        history = []
        
        while True:
            try:
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð¾Ñ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
                user_input = input("\nðŸ‘¤ Ð¢Ñ‹: ").strip()
                
                # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÐ¾Ð¼Ð°Ð½Ð´
                if user_input.lower() in ['/exit', '/quit', '/q', 'exit', 'quit', 'q']:
                    print("ðŸ‘‹ Ð”Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ!")
                    break
                
                elif user_input.lower() == '/clear':
                    history.clear()
                    print("ðŸ§¹ Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð°!")
                    continue
                
                elif user_input.lower().startswith('/temp '):
                    try:
                        temp = float(user_input.split()[1])
                        if 0.1 <= temp <= 2.0:
                            temperature = temp
                            print(f"ðŸŒ¡ï¸ Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°: {temperature}")
                        else:
                            print("âŒ Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð¾Ñ‚ 0.1 Ð´Ð¾ 2.0")
                    except:
                        print("âŒ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ: /temp 0.7")
                    continue
                
                elif user_input.lower().startswith('/len '):
                    try:
                        length = int(user_input.split()[1])
                        if 10 <= length <= 200:
                            max_length = length
                            print(f"ðŸ“ Ð”Ð»Ð¸Ð½Ð° Ð¾Ñ‚Ð²ÐµÑ‚Ð° ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°: {max_length}")
                        else:
                            print("âŒ Ð”Ð»Ð¸Ð½Ð° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð¾Ñ‚ 10 Ð´Ð¾ 200")
                    except:
                        print("âŒ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ: /len 100")
                    continue
                
                elif user_input.lower().startswith('/topk '):
                    try:
                        k = int(user_input.split()[1])
                        if 0 <= k <= 100:
                            top_k = k
                            print(f"ðŸŽ¯ Top-k ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½: {top_k}")
                        else:
                            print("âŒ Top-k Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¾Ñ‚ 0 Ð´Ð¾ 100")
                    except:
                        print("âŒ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ: /topk 50")
                    continue
                
                elif not user_input:
                    continue
                
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ
                history.append(f"ðŸ‘¤ Ð¢Ñ‹: {user_input}")
                
                # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ñ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ†Ð¸ÐµÐ¹ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°
                print("ðŸ¤– Ð‘Ð¾Ñ‚ Ð´ÑƒÐ¼Ð°ÐµÑ‚...", end=" ", flush=True)
                
                try:
                    response = self.generate(user_input, max_length, temperature, top_k)
                    print(f"\nðŸ¤– Ð‘Ð¾Ñ‚: {response}")
                    history.append(f"ðŸ¤– Ð‘Ð¾Ñ‚: {response}")
                except Exception as e:
                    print(f"\nâŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸: {e}")
                    continue
                
            except KeyboardInterrupt:
                print("\n\nðŸ‘‹ Ð”Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ!")
                break
            
            except Exception as e:
                print(f"\nâŒ ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}")
                continue
    
    def test_chat(self, num_turns=5):
        """Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ñ‡Ð°Ñ‚ Ñ Ð¿Ñ€ÐµÐ´Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°Ð¼Ð¸"""
        test_dialogue = [
            "ÐŸÑ€Ð¸Ð²ÐµÑ‚!",
            "ÐšÐ°Ðº Ñ‚ÐµÐ±Ñ Ð·Ð¾Ð²ÑƒÑ‚?",
            "Ð§Ñ‚Ð¾ Ñ‚Ñ‹ ÑƒÐ¼ÐµÐµÑˆÑŒ?",
            "Ð Ð°ÑÑÐºÐ°Ð¶Ð¸ Ñ‡Ñ‚Ð¾-Ð½Ð¸Ð±ÑƒÐ´ÑŒ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾Ðµ",
            "ÐŸÐ¾ÐºÐ°!"
        ]
        
        print("\n" + "="*60)
        print("ðŸ§ª Ð¢Ð•Ð¡Ð¢ÐžÐ’Ð«Ð™ Ð”Ð˜ÐÐ›ÐžÐ“".center(60))
        print("="*60)
        
        for i, prompt in enumerate(test_dialogue[:num_turns]):
            print(f"\nðŸ‘¤ Ð¢Ñ‹: {prompt}")
            response = self.generate(prompt, max_length=80, temperature=0.7)
            print(f"ðŸ¤– Ð‘Ð¾Ñ‚: {response}")
            
            if i < num_turns - 1:
                input("\nâŽ ÐÐ°Ð¶Ð¼Ð¸ Enter Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶ÐµÐ½Ð¸Ñ...")
        
        print("\n" + "="*60)
        print("âœ… Ð¢ÐµÑÑ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½!".center(60))
        print("="*60)

def main():
    parser = argparse.ArgumentParser(description="Ð§Ð°Ñ‚-Ð±Ð¾Ñ‚ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸")
    parser.add_argument("--model", type=str, default="trained_model", 
                       help="ÐŸÑƒÑ‚ÑŒ Ðº ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸")
    parser.add_argument("--chat", action="store_true", 
                       help="Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼")
    parser.add_argument("--test", action="store_true",
                       help="Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³")
    parser.add_argument("--prompt", type=str, 
                       help="ÐžÐ´Ð¸Ð½ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸")
    parser.add_argument("--temp", type=float, default=0.7,
                       help="Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ (0.1-2.0)")
    parser.add_argument("--len", type=int, default=100,
                       help="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð° Ð¾Ñ‚Ð²ÐµÑ‚Ð°")
    
    args = parser.parse_args()
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚Ð°
    try:
        bot = ChatBot(args.model)
    except Exception as e:
        print(f"âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {e}")
        return
    
    # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ€ÐµÐ¶Ð¸Ð¼ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹
    if args.chat:
        # Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ‡Ð°Ñ‚
        bot.interactive_chat()
    
    elif args.test:
        # Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³
        bot.test_chat()
    
    elif args.prompt:
        # ÐžÐ´Ð¸Ð½Ð¾Ñ‡Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ
        print(f"\nðŸŽ¯ ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚: {args.prompt}")
        response = bot.generate(args.prompt, max_length=args.len, temperature=args.temp)
        print(f"ðŸ¤– ÐžÑ‚Ð²ÐµÑ‚: {response}")
    
    else:
        # ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ Ð¸ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
        print("\n" + "="*60)
        print("ðŸ’¬ Ð§ÐÐ¢-Ð‘ÐžÐ¢ Ð“ÐžÐ¢ÐžÐ’ Ðš ÐžÐ‘Ð©Ð•ÐÐ˜Ð®".center(60))
        print("="*60)
        print("\nÐ’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ñ€ÐµÐ¶Ð¸Ð¼:")
        print("1. Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ‡Ð°Ñ‚")
        print("2. Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³")
        print("3. Ð’Ñ‹Ñ…Ð¾Ð´")
        
        choice = input("\nÐ’Ð°Ñˆ Ð²Ñ‹Ð±Ð¾Ñ€ (1-3): ").strip()
        
        if choice == "1":
            bot.interactive_chat()
        elif choice == "2":
            bot.test_chat()
        else:
            print("ðŸ‘‹ Ð”Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ!")

if __name__ == "__main__":
    main()